---
title: "Assignment 3 - Swimming pool"
author: "David Utassy"
date: '2021 02 12 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r includes, include=FALSE}
#########################################################################################
# Prepared for Gabor's Data Analysis
#
# Data Analysis for Business, Economics, and Policy
# by Gabor Bekes and  Gabor Kezdi
# Cambridge University Press 2021
#
# gabors-data-analysis.com 
#
# License: Free to share, modify and use for educational purposes. 
# 	Not to be used for commercial purposes.

# CHAPTER 18
# CH18 Forecasting daily ticket sales for a swimming pool 
# using swim data
# version 0.9 2020-08-31
#########################################################################################


#
###########################################################

# It is advised to start a new session for every case study
# Clear memory -------------------------------------------------------
rm(list=ls())

# Import libraries ---------------------------------------------------
library(tidyverse)
library(stargazer)
library(Hmisc)
library(timeDate)
library(lubridate)
library(caret)
library(StanHeaders)
library(prophet)
library(viridis)
library(ggpubr)
library(kableExtra)


setwd("/Users/utassydv/Documents/workspaces/CEU/my_repos/data_analysis_prediction/swimming_pool_forecasting")

data_in   <- "data/clean/"
data_out  <- "data/clean/"

# load theme and functions
source("code/helper/theme_bg.R")
source("code/helper/da_helper_functions.R")

#use_case_dir <- " swimmingpool/"
#ata_out <- use_case_dir
output <- "output/"
create_output_if_doesnt_exist(output)




#####################################
# Creating time features  ----------
#####################################


#import data
daily_agg<-read.csv(file = paste(data_in,"outdoor_pools_work.csv",sep="")) %>% 
  mutate(date = as.Date(date))

```


```{r start, include=FALSE, cache=TRUE}

Hmisc::describe(daily_agg)

# dow: 1=Monday, weekend: Sat and Sun.
daily_agg <- daily_agg %>%
  mutate(year = year(date),
         quarter = quarter(date),
         month = factor(month(date)),
         day = day(date)) %>%
  mutate(dow = factor(lubridate::wday(date, week_start = getOption("lubridate.week.start", 1)))) %>%
  mutate(weekend = factor(as.integer(dow %in% c(6,7))))


daily_agg <- daily_agg %>% 
  mutate(school_off = ((day>15 & month==5 & day <=30) | (month==6 |  month==7) |
                         (day<15 & month==8) | (day>20 & month==12) ))

daily_agg <- daily_agg %>% 
  mutate(trend = c(1:dim(daily_agg)[1]))

summary(daily_agg$QUANTITY)



# Get holiday calendar ----------------------------------

holidays <-  as.Date(holidayNYSE(2010:2017))
  
daily_agg <- daily_agg %>% 
  mutate(isHoliday = ifelse(date %in% holidays,1,0))

Hmisc::describe(daily_agg)

# Define vars for analysis ----------------------------------

daily_agg <- 
  daily_agg %>% 
  group_by(month) %>% 
  mutate(q_month = mean(QUANTITY)) %>% 
  ungroup()

daily_agg <- daily_agg %>% 
  mutate(QUANTITY2 = ifelse(QUANTITY<1, 1, QUANTITY)) %>% 
  mutate(q_ln = log(QUANTITY2))

daily_agg <- 
  daily_agg %>% 
  group_by(month, dow) %>% 
  mutate(tickets = mean(QUANTITY),
         tickets_ln = mean(q_ln)) %>% 
  ungroup()

# named date vars for graphs
mydays <- c("Mon","Tue","Wed",
            "Thu","Fri","Sat",
            "Sun")
daily_agg$dow_abb   <-factor(   mydays[daily_agg$dow],  levels=mydays)
daily_agg$month_abb <-factor(month.abb[daily_agg$month],levels=month.abb)

################################
# Descriptive graphs ----------
#################################


g1 <-ggplot(data=daily_agg[daily_agg$year==2015,], aes(x=date, y=QUANTITY)) +
  geom_line(size=0.4, color=color[1]) +
  theme_bg() +
  scale_x_date(breaks = as.Date(c("2015-01-01","2015-04-01","2015-07-01","2015-10-01","2016-01-01")),
               labels = date_format("%d%b%Y"),
               date_minor_breaks = "1 month" ) +
  labs( x = "Date (day)", y="Daily ticket sales" ) +
  scale_color_discrete(name = "") +
  theme(axis.title.x=element_text(size=12),
        axis.title.y=element_text(size=12))
g1
save_fig(" figure-3a-swimmingpool-2015", output, "small")

g2<-ggplot(data=daily_agg[(daily_agg$year>=2010) & (daily_agg$year<=2014),], aes(x=date, y=QUANTITY)) +
  geom_line(size=0.2, color=color[1]) +
  theme_bg() +
  scale_x_date(breaks = as.Date(c("2010-01-01","2011-01-01","2012-01-01","2013-01-01","2014-01-01","2015-01-01")),
               labels = date_format("%d%b%Y"),
               minor_breaks = "3 months") +
  labs( x = "Date (day)", y="Daily ticket sales" ) +
  scale_color_discrete(name = "")+
  theme(axis.title.x=element_text(size=12),
        axis.title.y=element_text(size=12))
g2
save_fig(" figure-3b-swimmingpool-2010-2014", output, "small")


g3<-ggplot(data=daily_agg, aes(x=month_abb, y=QUANTITY)) +
  theme_bg() +
  labs( x = "Date (month)", y="Daily ticket sales" ) +
  geom_boxplot(color=color[1],outlier.color = color[4], outlier.alpha = 0.6, outlier.size = 0.4)+
  theme(axis.title.x=element_text(size=12),
        axis.title.y=element_text(size=12))
g3
save_fig(" figure-4a-swimmingpool-monthly", output, "small")

g4<-ggplot(data=daily_agg, aes(x=dow_abb, y=QUANTITY)) +
  theme_bg() +
  labs( x = "Day of the week", y="Daily ticket sales" ) +
  geom_boxplot(color=color[1],outlier.color = color[4], outlier.alpha = 0.6, outlier.size = 0.4)+
  theme(axis.title.x=element_text(size=12),
        axis.title.y=element_text(size=12))
  #geom_boxplot(color=color[1], outlier.shape = NA)
g4
save_fig(" figure-4b-swimmingpool-dow", output, "small")

# to check for interactions, look at the heatmap
swim_heatmap <- 
  ggplot(daily_agg, aes(x = dow_abb, y = month_abb, fill = tickets)) +
  geom_tile(colour = "white") +
  labs(x = 'Day of the week', y = 'Month ') +
  scale_fill_viridis(alpha = 0.7, begin = 1, end = 0.2, direction = 1, option = "D") +
  theme_bg() +
  theme(legend.position = "right",
    legend.text = element_text(size=6),
    legend.title =element_text(size=6)
    )
swim_heatmap
save_fig(" figure-5-swim-heatmap", output, "large")

# not in book
swim_heatmap_log <-
  ggplot(daily_agg, aes(x = dow_abb, y = month_abb, fill = tickets_ln)) +
  geom_tile(colour = "white") +
  labs(x = 'Day of week', y = 'Month ') +
  scale_fill_viridis(alpha = 0.7, begin = 1, end = 0.2, direction = 1, option = "D") +
  theme_bg() +
  theme(legend.position = "right",
    legend.text = element_text(size=6),
    legend.title =element_text(size=6)
  )  
swim_heatmap_log

#####################################
# PREDICTION  ----------
#####################################


#############################
# Create train/houldout data
#############################

# Last year of data
data_holdout<- daily_agg %>%
  filter(year==2016)

# Rest of data for training
data_train <- daily_agg %>%
  filter(year<2016)

# Prepare for cross-validation
data_train <- data_train %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.integer(rowname))

test_index_list <- data_train %>% 
  split(f = factor(data_train$year)) %>% 
  lapply(FUN = function(x){x$rowname})
  
train_index_list <- test_index_list %>% 
  lapply(FUN = function(x){setdiff(data_train$rowname, x)})
  
train_control <- trainControl(
  method = "cv",
  index = train_index_list, #index of train data for each fold
  # indexOut = index of test data for each fold, complement of index by default
  # indexFinal = index of data to use to train final model, whole train data by default
  savePredictions = TRUE
)

# Fit models ---------------------------------------------------------

#Model 1 linear trend + monthly seasonality
model1 <- as.formula(QUANTITY ~ 1 + trend + month)
reg1 <- train(
  model1,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 2 linear trend + monthly seasonality + days of week seasonality 
model2 <- as.formula(QUANTITY ~ 1 + trend + month + dow)
reg2 <- train(
  model2,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 3 linear trend + monthly seasonality + days of week  seasonality + holidays 
model3 <- as.formula(QUANTITY ~ 1 + trend + month + dow + isHoliday)
reg3 <- train(
  model3,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 4 linear trend + monthly seasonality + days of week  seasonality + holidays + sch*dow
model4 <- as.formula(QUANTITY ~ 1 + trend + month + dow + isHoliday + school_off*dow)
reg4 <- train(
  model4,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 5 linear trend + monthly seasonality + days of week  seasonality + holidays + interactions
model5 <- as.formula(QUANTITY ~ 1 + trend + month + dow + isHoliday + school_off*dow + weekend*month)
reg5 <- train(
  model5,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 6 =  multiplicative trend and seasonality (ie take logs, predict log values and transform back with correction term)
model6 <- as.formula(q_ln ~ 1 + trend + month + dow + isHoliday + school_off*dow)
reg6 <- train(
  model6,
  method = "lm",
  data = data_train,
  trControl = train_control
)


stargazer(reg2$finalModel, reg3$finalModel, reg4$finalModel, reg5$finalModel, 
          out=paste(output,"Ch18_swim_tsregs.txt",sep=""), type = "text", digits=2)
stargazer(reg6$finalModel, 
          out=paste(output,"Ch18_swim_tsregs2.txt",sep=""), type = "text", digits=2)

# Get CV RMSE ----------------------------------------------

model_names <- c("reg1","reg2","reg3","reg4","reg5")
rmse_CV <- c()

for (i in model_names) {
  rmse_CV[i]  <- get(i)$results$RMSE
}
rmse_CV

#had to cheat and use train error on full train set because could not obtain CV fold train errors
corrb <- mean((reg6$finalModel$residuals)^2)
rmse_CV["reg6"] <- reg6$pred %>% 
  mutate(pred = exp(pred  + corrb/2)) %>% 
  group_by(Resample) %>% 
  summarise(rmse = RMSE(pred, exp(obs))) %>% 
  as.data.frame() %>% 
  summarise(mean(rmse)) %>% 
  as.numeric()
rmse_CV["reg6"] 

# Use prophet prediction -------------------------------------------
# add CV into prophet
# can be done with prophet: https://facebook.github.io/prophet/docs/diagnostics.html
# done but this is a different cross-validation as for the other models as it must be time-series like

# prophet -  multiplicative option -- tried but produced much worse results (~34. RMSE)


model_prophet <- prophet( fit=F, 
                          seasonality.mode = "additive", 
                          yearly.seasonality = "auto",
                          weekly.seasonality = "auto",
                          growth = "linear",
                          daily.seasonality=TRUE)

model_prophet <-  add_country_holidays(model_prophet, "US")
model_prophet <- fit.prophet(model_prophet, df= data.frame(ds = data_train$date,
                                                           y = data_train$QUANTITY ))

cv_pred <- cross_validation(model_prophet, initial = 365, period = 365, horizon = 365, units = 'days')
rmse_prophet_cv <- performance_metrics(cv_pred, rolling_window = 1)$rmse
rmse_prophet_cv



###########################x
# Evaluate best model on holdout set --------------------------------------------
###########################

future <- make_future_dataframe(model_prophet, include_history = F, periods = 366)
y_hat_prop = predict(model_prophet, future)
tail(y_hat_prop[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')])


data_holdout <- data_holdout %>% 
  mutate(y_hat_5 = predict(reg5, newdata = .)) %>% 
  mutate(y_hat_prop = predict(model_prophet, future)$yhat)

rmse_holdout_5 <- RMSE(data_holdout$QUANTITY, data_holdout$y_hat_5)
rmse_holdout_5

rmse_holdout_prop <- RMSE(data_holdout$QUANTITY, data_holdout$y_hat_prop)
rmse_holdout_prop



###########################x
# Plot best predictions --------------------------------------------
###########################x

#graph relative RMSE (on holdout) per month 
rmse_monthly <- data_holdout %>% 
  mutate(month = factor(format(date,"%b"), 
                        levels= unique(format(sort(.$date),"%b")), 
                        ordered=TRUE)) %>% 
  group_by(month) %>% 
  summarise(
    RMSE_5 = RMSE(QUANTITY, y_hat_5),
    RMSE_5_norm= RMSE(QUANTITY, y_hat_5)/mean(QUANTITY),
    RMSE_prop = RMSE(QUANTITY, y_hat_prop),
    RMSE_prop_norm= RMSE(QUANTITY, y_hat_prop)/mean(QUANTITY)
            ) 

library(reshape2)
rmse_monthly_long <- reshape2::melt(rmse_monthly, id.vars = "month")
rmse_monthly_long <- rmse_monthly_long[rmse_monthly_long$variable %in% c("RMSE_5_norm","RMSE_prop_norm"),]

rmse_monthly_long <- rmse_monthly_long %>%
  mutate( variable = ifelse(variable == "RMSE_5_norm", "Model 5", "Model Prophet"))

g_predictions_rmse<- ggplot(rmse_monthly_long, aes(x = month, y = value, fill = variable)) +
  geom_col(position = "dodge") +
  labs( x = "Date (month)", y="RMSE (normalized by monthly sales)", fill="Models") +
  theme_bg() +
  theme(legend.position = c(0.5, 0.7), legend.title = element_text(colour="Black", size=16, face="bold"),
        legend.text=element_text(size=8))
g_predictions_rmse
#save_fig("ch18_swim_predictions_rmse", output, "small")
save_fig(" figure-7b-swim-predictions-rmse", output, "small", plot=g_predictions_rmse)

g_predictions<-
  ggplot(data=data_holdout, aes(x=date, y=QUANTITY)) +
  geom_line(aes(size="Actual", colour="Actual", linetype = "Actual") ) +
  geom_line(aes(y=y_hat_prop, size="Predicted" ,colour="Predicted",  linetype= "Predicted")) +
  scale_y_continuous(expand = c(0,0))+
  scale_x_date(expand=c(0,0), breaks = as.Date(c("2016-01-01","2016-03-01","2016-05-01","2016-07-01","2016-09-01","2016-11-01", "2017-01-01")),
               labels = date_format("%d%b%Y"),
               date_minor_breaks = "1 month" )+
  scale_color_manual(values=color[1:2], name="")+
  scale_size_manual(name="", values=c(0.4,0.7))+
  #scale_linetype_manual(name = "", values=c("solid", "solid")) +
  scale_linetype_manual(name = "", values=c("solid", "twodash")) +
  labs( x = "Date (day)", y="Daily ticket sales" ) +
  theme_bg() +
  #theme(legend.position = "none") +
  #annotate("text", x = as.Date("2016-07-15"), y = 50, label = "Predicted", color=color[2], size=3)+
  #annotate("text", x = as.Date("2016-09-01"), y = 125, label = "Actual", color=color[1], size=3)
  theme(legend.position=c(0.8,0.8),
      legend.direction = "vertical",
      legend.text = element_text(size = 12),
      legend.key.width = unit(.8, "cm"),
      legend.key.height = unit(.3, "cm"),
      axis.title.x=element_text(size=12),
      axis.title.y=element_text(size=12)) + 
  guides(linetype = guide_legend(override.aes = list(size = 1.0))
         )
g_predictions
#save_fig("ch18_swim_predictions", output, "large")
save_fig(" figure-6-swim-predictions", output, "large", plot=g_predictions)


g_predictions_m <- ggplot(data=data_holdout %>% filter(month==8), aes(x=date, y=QUANTITY)) +
  geom_line(aes(size="Actual", colour="Actual", linetype = "Actual") ) +
  geom_line(aes(y=y_hat_prop, size="Predicted" ,colour="Predicted",  linetype= "Predicted")) +
  geom_ribbon(aes(ymin=QUANTITY,ymax=y_hat_prop), fill=color[4], alpha=0.3) +
  scale_y_continuous(expand = c(0.01,0.01), limits = c(-40,800))+
  scale_x_date(expand=c(0.01,0.01), breaks = as.Date(c("2016-08-01","2016-08-08","2016-08-15","2016-08-22","2016-08-29")),
               limits = as.Date(c("2016-08-01","2016-08-31")),
               labels = date_format("%d%b")) +
  scale_color_manual(values=color[1:2], name="")+
  scale_size_manual(name="", values=c(0.4,0.7))+
  #scale_linetype_manual(name = "", values=c("solid", "solid")) +
  scale_linetype_manual(name = "", values=c("solid", "twodash")) +
  labs( x = "Date (day)", y="Daily ticket sales" ) +
  theme_bg() +
  #theme(legend.position = "none") +
  #annotate("text", x = as.Date("2016-08-04"), y = 55, label = "Actual", color=color[2], size=2)+
  #annotate("text", x = as.Date("2016-08-17"), y = 115, label = "Predicted", color=color[1], size=2)
  theme(legend.position=c(0.7,0.8),
        legend.direction = "vertical",
        legend.text = element_text(size = 12),
        legend.key.width = unit(.8, "cm"),
        legend.key.height = unit(.2, "cm"),
      axis.title.x=element_text(size=12),
      axis.title.y=element_text(size=12)) + 
  guides(linetype = guide_legend(override.aes = list(size = 1.0))
  )
g_predictions_m
#save_fig("ch18_swim_predictions_m", output, "small")
save_fig(" figure-7a-swim-predictions-m", output, "small", plot=g_predictions_m)

# table-1-swim-rmse
# table-2-cs-models-rmse
# table-3-arima-folds

rmse_CV$prophet <- rmse_prophet_cv
rmse_df <- data.frame(rmse_CV)
rmse_df <-  t(rmse_df)
colnames(rmse_df) <- "CV RMSE"
rownames(rmse_df) <- c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5", "Model 6", "Prophet")
```

## Introduction

The purpose of this writing is to document the third assignment for the course Data Analysis 3 course. 

The task was to build a model to forecast 12 months ahead on swimming pool ticket transactions.

The whole analysis was written in R, and the code is available under this [GitHub repository](https://github.com/utassydv/data_analysis_prediction/tree/main/swimming_pool_forecasting). We can separate the code into three parts. I prepared the data mainly in the "swim-transactions-cleaner.R" file, then the analysis and model building is done in the "swimmingpool-predict.R" file, and the "swimmingpool-predict-doc.Rmd" file is responsible for creating this document itself. 

## Data

This project is similar to the case study in CH. 13.  [Data Analysis for Business, Economics, and Policy, Gabor Bekes and Gabor Kezdi](https://gabors-data-analysis.com/). 

The project uses [the publicly available dataset](https://drive.google.com/drive/u/0/folders/1MTtSq7SlZwwkOt7MZSwylcpOXO75Dksv) from the city called Albuquerque (New Mexico), which was collected by POS terminals in the local swimming pools.

#### Data preparation

The original data is transaction-level ticket sales for each swimming pool in the city. In contrast with the case study in the above-mentioned book, my goal was to train my model on all the open-air pools in the city. I used the official listing of [outdoor swimming pools in the city](https://www.cabq.gov/parksandrecreation/recreation/swimming/outdoor-pools) and intuitively matched them with the location codes in the provided data. 

After some explanatory data analysis, I made some modifications on the cleaner code, that the book uses to prepare data. The main aim of this data preparation is to aggregate ticket sales into a daily level. 

Instead of filtering to one specific swimming pool, I filtered for all outdoor swimming pools with their Location IDs. (7 pools)
I added “ADMINISTER 3’ as an included category

In the upcoming graphs, it can be clearly seen, that the daily data has a strong seasonality.

```{r EDA, fig.width=8,fig.height=4, fig.align='center', echo = FALSE , results = 'asis', warning = FALSE, message = FALSE}
ggarrange(g1, g2,  ncol = 2, nrow = 1)
```

Furthermore, to capture monthly and day of the week seasonalities the following box-plots can help us. On the monthly plot we can see, that summer is an outstanding season, and the daily plot shows, that Fridays are outstanding as well.

```{r boxplots, fig.width=8,fig.height=4, fig.align='center', echo = FALSE , results = 'asis', warning = FALSE, message = FALSE}
ggarrange(g3, g4,  ncol = 2, nrow = 1)
```

To combine day-of-week and monthly variables as interactions we have to have a better plot, that can show us the connections. I used the same heatmap as in the case study in order to observe these interactions. 

```{r heatmap, fig.width=8,fig.height=4, fig.align='center', echo = FALSE , results = 'asis', warning = FALSE, message = FALSE}
swim_heatmap
```

From the heatmap, we can say the same things as in the case study. I would highlight only one difference, that my data (containing all out-door pools) has a less significant drop on Fridays compared to the specific Sunport pool. 

## Model Building for probability prediction

This is a long-horizon forecasting task, as I have to predict one year ahead on a daily level (365 days).

Just as in the case study on the above-mentioned book I built 7 different models:

1) linear trend + monthly seasonality
2) linear trend + monthly seasonality + days of week seasonality 
3) linear trend + monthly seasonality + days of week seasonality + holidays 
4) linear trend + monthly seasonality + days of week seasonality + holidays + school holiday * day-of-week 
5) linear trend + monthly seasonality + days of week  seasonality + holidays + interactions
6) multiplicative trend and seasonality (logs, predict log values and transform back with correction term)
7) prophet [facebook’s forecasting tool](https://facebook.github.io/prophet/docs/diagnostics.html)

As a holdout dataset, I separated 2016, which is the latest year in the dataset for final evaluation. In order to compare the models, I made 5 fold cross-validation. In the table below we can see the result of this comparison. We can see that the best models are Model 5 and the model created by Prophet. In contrast with the case study in the book, prophet outperformed my models. 

```{r cv_rmse, echo = FALSE , results = "asis", warning = FALSE, message = FALSE }
kbl(rmse_df, digits = 2)  %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```

To compare the two best models I compared their cross-validated RMSE monthly, which can be observed on the following plot. It is interesting to see, that Model 5 performed better in most of the months but had a huge error in December. This means that Model 5 could be a better model if we can fix it in December. Possibly we should make some modifications with winter holiday dates in order to make it better.

```{r comp, fig.width=8,fig.height=4, fig.align='center', echo = FALSE , results = 'asis', warning = FALSE, message = FALSE}
g_predictions_rmse
```

However, on average the model created by Prophet is still better, therefore I will go on with that one.

## Model Evaluation

To evaluate the best model (created by Prophet) I used the holdout dataset (2016). The RMSE turned out to be 86.2 which is a strong result compared to the CV results. (Model 5 gave 99.4)

In order to visualize the results, I created two plots representing the actual and predicted quantities in December 2016 and in the whole year. We can see, that our model catches to most important trends, and produces a decent result, however, it misses sudden spikes.

```{r eval, fig.width=8,fig.height=4, fig.align='center', echo = FALSE , results = 'asis', warning = FALSE, message = FALSE}
ggarrange(g_predictions, g_predictions_m,  ncol = 2, nrow = 1)
```

# Summary

To conclude, I would like to highlight, that my assignment using 7 standalone pools is different from the case study as its data is already less random as different pools are smoothing out the aggregated data, however, a single pool would have some specific extreme values in the data. Furthermore, it is clear, that the RMSE in my assignment is about 3 times higher than in the case study, but the quantity of the tickets sold is on average 5 times higher, which means that it is easier to predict on 7 outdoor pools on average compared to a single one. 






